{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from model import load_model, detect\n",
    "from inputs import sports_input\n",
    "from baseline import BaselineAttack\n",
    "from discrete_alteration import DiscreteAlterations\n",
    "from paraphrasing import ParaphrasingAttack\n",
    "from tokenization import TokenizationAttack\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./watermarked_outputs/sports_watermarked.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_token_mappings():\n",
    "    total_tokens = 100000\n",
    "    topics = [\"sports\", \"animals\", \"technology\", \"music\", \"medicine\"]\n",
    "    topic_token_mapping = {topic: [] for topic in topics}\n",
    "\n",
    "    for i in range(total_tokens):\n",
    "        topic_index = i % len(topics)\n",
    "        topic = topics[topic_index]\n",
    "        topic_token_mapping[topic].append(i)\n",
    "    return topic_token_mapping\n",
    "token_mappings = get_token_mappings()\n",
    "\n",
    "args = {\n",
    "    'demo_public': False, \n",
    "    'model_name_or_path': 'facebook/opt-1.3b', \n",
    "    'load_fp16' : False,\n",
    "    'prompt_max_length': None, \n",
    "    'max_new_tokens': 200, \n",
    "    'generation_seed': 123, \n",
    "    'use_sampling': True, \n",
    "    'n_beams': 1, \n",
    "    'sampling_temp': 0.7, \n",
    "    'use_gpu': True, \n",
    "    'seeding_scheme': 'simple_1', \n",
    "    'gamma': 0.25, \n",
    "    'delta': 2.0, \n",
    "    'normalizers': '', \n",
    "    'ignore_repeated_bigrams': False, \n",
    "    'detection_z_threshold': 2.0, \n",
    "    'select_green_tokens': True,\n",
    "    'skip_model_load': False,\n",
    "    'seed_separately': True,\n",
    "    'is_topic': False,\n",
    "    'topic_token_mapping': token_mappings,\n",
    "    'detected_topic': \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = sports_input()\n",
    "model, tokenizer = load_model(args)\n",
    "\n",
    "input_prompt = input_text + content\n",
    "\n",
    "without_watermark_detection_result = detect(input_prompt, content, \n",
    "                                        args, \n",
    "                                        device=device, \n",
    "                                        tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "print(f\"Original Detection: \\n{without_watermark_detection_result}\")\n",
    "for item in without_watermark_detection_result:\n",
    "    if len(item) > 0:\n",
    "        if item[0] == 'z-score':\n",
    "            original_z_score = float(item[1])\n",
    "        elif item[0] == 'Tokens Counted (T)':\n",
    "            total_tokens = int(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = BaselineAttack()\n",
    "n_modifications = list(range(0, 51, 2))\n",
    "\n",
    "z_score_results = {'insertion': [], 'deletion': [], 'substitution': [], 'combination': [], 'inference_insertion': [], 'inference_deletion': [], 'inference_substitution': [], 'inference_combination': []}\n",
    "robustness_scores = {'insertion': [], 'deletion': [], 'substitution': [], 'combination': [], 'inference_insertion': [], 'inference_deletion': [], 'inference_substitution': [], 'inference_combination': []}\n",
    "\n",
    "def evaluate_attack(attack_type, n_edits, inference=False):\n",
    "    if attack_type == 'insertion':\n",
    "        if inference:\n",
    "            attacked_text = baseline.inference_modify_text(content, n_edits, edit_type='insert')\n",
    "        else:\n",
    "            attacked_text = baseline.modify_text(content, n_edits, edit_type='insert')\n",
    "    elif attack_type == 'deletion':\n",
    "        if inference:\n",
    "            attacked_text = baseline.inference_modify_text(content, n_edits, edit_type='delete')\n",
    "        else:\n",
    "            attacked_text = baseline.modify_text(content, n_edits, edit_type='delete')\n",
    "    elif attack_type == 'substitution':\n",
    "        if inference:\n",
    "            attacked_text = baseline.inference_modify_text(content, n_edits, edit_type='substitute')\n",
    "        else:\n",
    "            attacked_text = baseline.modify_text(content, n_edits, edit_type='substitute')\n",
    "    elif attack_type == 'combination':\n",
    "        if inference:\n",
    "            attacked_text = baseline.combination_modify_text(content, insertion_n_edits=n_edits//3, insertion_is_inferenced=True, deletion_n_edits=n_edits//3, deletion_is_inferenced=True, substitution_n_edits=n_edits//3, substitution_is_inferenced=True)\n",
    "        else:\n",
    "            attacked_text = baseline.combination_modify_text(content, insertion_n_edits=n_edits//3, deletion_n_edits=n_edits//3, substitution_n_edits=n_edits//3)\n",
    "    return attacked_text\n",
    "\n",
    "for n in n_modifications:\n",
    "    print(n)\n",
    "    for attack_type in ['insertion', 'deletion', 'substitution', 'combination']:\n",
    "        print(attack_type)\n",
    "        attacked_text = evaluate_attack(attack_type, n)\n",
    "        attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "        print('detection finished')\n",
    "\n",
    "        for item in attacked_detection_result:\n",
    "            if len(item) > 0:\n",
    "                if item[0] == 'z-score':\n",
    "                    z_score = float(item[1])\n",
    "                    break\n",
    "        z_score_results[attack_type].append(z_score)\n",
    "        print(z_score_results)\n",
    "\n",
    "        if n > 0:  \n",
    "            robustness_score = (original_z_score - z_score) / n\n",
    "        else:\n",
    "            robustness_score = 0\n",
    "\n",
    "        robustness_scores[attack_type].append(robustness_score)\n",
    "\n",
    "        inference_attack_type = f'inference_{attack_type}'\n",
    "        attacked_text = evaluate_attack(attack_type, n, inference=True)\n",
    "        attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "        for item in attacked_detection_result:\n",
    "            if len(item) > 0:\n",
    "                if item[0] == 'z-score':\n",
    "                    z_score = float(item[1])\n",
    "                    break\n",
    "        z_score_results[inference_attack_type].append(z_score)\n",
    "        if n > 0:\n",
    "            robustness_score = (original_z_score - z_score) / n\n",
    "        else:\n",
    "            robustness_score = 0\n",
    "        robustness_scores[inference_attack_type].append(robustness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "colors = [\n",
    "    '#FF9999', '#66B2FF', '#99FF99', '#FFCC99',\n",
    "    '#FFD700', '#FF69B4', '#87CEEB', '#D8BFD8'\n",
    "]\n",
    "\n",
    "for i, (attack_type, z_scores) in enumerate(z_score_results.items()):\n",
    "    percentage_modifications = [(modifications / total_tokens) * 100 for modifications in n_modifications]\n",
    "    plt.plot(percentage_modifications, z_scores, \n",
    "             label=f'{attack_type.replace(\"_\", \" \").title()} Attack',\n",
    "             color=colors[i % len(colors)], \n",
    "             marker='o', \n",
    "             linewidth=2,\n",
    "             markersize=6)\n",
    "    \n",
    "plt.title('Baseline Attack Z-Score vs. Percentage of Modifications')\n",
    "plt.xlabel('Percentage of Modifications (%)')\n",
    "plt.ylabel('Z-Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "for x in np.arange(min(percentage_modifications), max(percentage_modifications) + 1, 5):\n",
    "    plt.axvline(x=x, color='grey', linestyle='--', linewidth=0.5)\n",
    "for y in np.arange(0, max(max(z_scores) for z_scores in z_score_results.values()) + 0.25, 0.25):\n",
    "    plt.axhline(y=y, color='grey', linestyle='--', linewidth=0.5)\n",
    "rect = plt.Rectangle((0, 0), 1, 1, color='grey', alpha=0.2, transform=plt.gca().transAxes, zorder=-1)\n",
    "plt.gca().add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "colors = [\n",
    "    '#FF9999', '#66B2FF', '#99FF99', '#FFCC99',\n",
    "    '#FFD700', '#FF69B4', '#87CEEB', '#D8BFD8'\n",
    "]\n",
    "\n",
    "for i, (attack_type, scores) in enumerate(robustness_scores.items()):\n",
    "    percentage_modifications = [(modifications / total_tokens) * 100 for modifications in n_modifications]\n",
    "    plt.plot(percentage_modifications, scores, \n",
    "             label=f'{attack_type.replace(\"_\", \" \").title()} Attack',\n",
    "             color=colors[i % len(colors)],\n",
    "             marker='o',\n",
    "             linewidth=2,\n",
    "             markersize=6)\n",
    "\n",
    "plt.title('Baseline Attack Robustness Score vs. Percentage of Modifications')\n",
    "plt.xlabel('Percentage of Modifications (%)')\n",
    "plt.ylabel('Robustness Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "for x in np.arange(min(percentage_modifications), max(percentage_modifications) + 1, 5):\n",
    "    plt.axvline(x=x, color='grey', linestyle='--', linewidth=0.5)\n",
    "for y in np.arange(0, max(max(scores) for scores in robustness_scores.values()) + 0.25, 0.25):\n",
    "    plt.axhline(y=y, color='grey', linestyle='--', linewidth=0.5)\n",
    "rect = plt.Rectangle((0, 0), 1, 1, color='grey', alpha=0.2, transform=plt.gca().transAxes, zorder=-1)\n",
    "plt.gca().add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_modifications = list(range(0, 51, 2))\n",
    "\n",
    "z_score_results = {'whitespace': [], 'add_char': [], 'combination': [], 'inference_whitespace': [], 'inference_add_char': [], 'inference_combination': []}\n",
    "robustness_scores = {'whitespace': [], 'add_char': [], 'combination': [], 'inference_whitespace': [], 'inference_add_char': [], 'inference_combination': []}\n",
    "\n",
    "alteration = DiscreteAlterations()\n",
    "\n",
    "def evaluate_discrete_attack(attack_type, n_edits, inference=False):\n",
    "    if attack_type == 'whitespace':\n",
    "        attacked_text = alteration.add_whitespace(content, n_edits, inference=inference)\n",
    "    elif attack_type == 'add_char':\n",
    "        attacked_text = alteration.add_char(content, n_edits, inference=inference)\n",
    "    elif attack_type == 'combination':\n",
    "        attacked_text = alteration.combination_modify_text(content, whitespace_n_edits=n_edits//2, white_space_inference=inference, add_char_n_edits=n_edits//2, add_char_inference=inference)\n",
    "    return attacked_text\n",
    "\n",
    "\n",
    "for n in n_modifications:\n",
    "    print(n)\n",
    "    for attack_type in ['whitespace', 'add_char', 'combination']:\n",
    "        print(attack_type)\n",
    "\n",
    "        attacked_text = evaluate_discrete_attack(attack_type, n)\n",
    "        attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "        print('non-inference detection finished')\n",
    "\n",
    "        for item in attacked_detection_result:\n",
    "            if len(item) > 0:\n",
    "                if item[0] == 'z-score':\n",
    "                    z_score = float(item[1])\n",
    "                    break\n",
    "        z_score_results[attack_type].append(z_score)\n",
    "        if n > 0:\n",
    "            robustness_score = (original_z_score - z_score) / n\n",
    "        else:\n",
    "            robustness_score = 0\n",
    "        robustness_scores[attack_type].append(robustness_score)\n",
    "\n",
    "        inference_attack_type = f'inference_{attack_type}'\n",
    "        attacked_text = evaluate_discrete_attack(attack_type, n, inference=True)\n",
    "        attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "        print('inference detection finished')\n",
    "        \n",
    "        for item in attacked_detection_result:\n",
    "            if len(item) > 0:\n",
    "                if item[0] == 'z-score':\n",
    "                    z_score = float(item[1])\n",
    "                    break\n",
    "        z_score_results[inference_attack_type].append(z_score)\n",
    "        if n > 0:\n",
    "            robustness_score = (original_z_score - z_score) / n\n",
    "        else:\n",
    "            robustness_score = 0\n",
    "        robustness_scores[inference_attack_type].append(robustness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "label_map = {\n",
    "    'add_char': 'Add Character',\n",
    "    'inference_add_char': 'Inference Add Character'\n",
    "}\n",
    "for i, (attack_type, z_scores) in enumerate(z_score_results.items()):\n",
    "    percentage_modifications = [(modifications / total_tokens) * 100 for modifications in n_modifications]\n",
    "    plt.plot(percentage_modifications, z_scores, \n",
    "             label = label_map.get(attack_type, attack_type.replace(\"_\", \" \").title()) + \" Attack\",\n",
    "             color=colors[i % len(colors)],  \n",
    "             marker='o', \n",
    "             linewidth=2,  \n",
    "             markersize=6)  \n",
    "\n",
    "plt.title('Discrete Alterations Z-Score vs. Percentage of Modifications')\n",
    "plt.xlabel('Pecentage of Modifications (%)')\n",
    "plt.ylabel('Z-Score')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "\n",
    "for x in np.arange(min(percentage_modifications), max(percentage_modifications) + 1, 5):\n",
    "    plt.axvline(x=x, color='grey', linestyle='--', linewidth=0.5)\n",
    "for y in np.arange(0, max(max(z_scores) for z_scores in z_score_results.values()) + 0.25, 0.25):\n",
    "    plt.axhline(y=y, color='grey', linestyle='--', linewidth=0.5)\n",
    "rect = plt.Rectangle((0, 0), 1, 1, color='grey', alpha=0.2, transform=plt.gca().transAxes, zorder=-1)\n",
    "plt.gca().add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i, (attack_type, scores) in enumerate(robustness_scores.items()):\n",
    "    label = label_map.get(attack_type, attack_type.replace(\"_\", \" \").title()) + \" Attack\" \n",
    "    percentage_modifications = [(modifications / total_tokens) * 100 for modifications in n_modifications]\n",
    "    plt.plot(percentage_modifications, scores,\n",
    "             label=label, \n",
    "             color=colors[i % len(colors)],  \n",
    "             marker='o', \n",
    "             linewidth=2,\n",
    "             markersize=6)\n",
    "\n",
    "plt.title('Discrete Alterations Robustness Score vs. Percentage of Modifications')\n",
    "plt.xlabel('Percentage of Modifications (%)')\n",
    "plt.ylabel('Robustness Score')\n",
    "plt.legend(loc='upper right') \n",
    "plt.grid(True)\n",
    "\n",
    "for x in np.arange(min(percentage_modifications), max(percentage_modifications) + 1, 5):\n",
    "    plt.axvline(x=x, color='grey', linestyle='--', linewidth=0.5)\n",
    "for y in np.arange(0, max(max(scores) for scores in robustness_scores.values()) + 0.25, 0.25):\n",
    "    plt.axhline(y=y, color='grey', linestyle='--', linewidth=0.5)\n",
    "rect = plt.Rectangle((0, 0), 1, 1, color='grey', alpha=0.2, transform=plt.gca().transAxes, zorder=-1)\n",
    "plt.gca().add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_modifications = list(range(0, 51, 2))\n",
    "\n",
    "z_score_results = {'tokenization': [], 'inference_tokenization': []}\n",
    "robustness_scores = {'tokenization': [], 'inference_tokenization': []}\n",
    "\n",
    "tokenization = TokenizationAttack()\n",
    "\n",
    "def evaluate_tokenization_attack(n_edits, inference=False):\n",
    "    attacked_text = tokenization.tokenization_attack(content, n_edits, inference=inference)\n",
    "    return attacked_text\n",
    "\n",
    "\n",
    "for n in n_modifications:\n",
    "    print(n)\n",
    "    attacked_text = evaluate_tokenization_attack(n)\n",
    "    attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "    print('detected non-inference')\n",
    "    for item in attacked_detection_result:\n",
    "            if len(item) > 0:\n",
    "                if item[0] == 'z-score':\n",
    "                    z_score = float(item[1])\n",
    "                    break\n",
    "    z_score_results['tokenization'].append(z_score)\n",
    "    if n > 0:\n",
    "        robustness_score = (original_z_score - z_score) / n\n",
    "    else:\n",
    "        robustness_score = 0\n",
    "    robustness_scores['tokenization'].append(robustness_score)\n",
    "\n",
    "    attacked_text = evaluate_tokenization_attack(n, inference=True)\n",
    "    attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "    print('detected inference')\n",
    "    for item in attacked_detection_result:\n",
    "            if len(item) > 0:\n",
    "                if item[0] == 'z-score':\n",
    "                    z_score = float(item[1])\n",
    "                    break\n",
    "    z_score_results['inference_tokenization'].append(z_score)\n",
    "    if n > 0:\n",
    "        robustness_score = (original_z_score - z_score) / n\n",
    "    else:\n",
    "        robustness_score = 0\n",
    "    robustness_scores['inference_tokenization'].append(robustness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i, (attack_type, z_scores) in enumerate(z_score_results.items()):\n",
    "    percentage_modifications = [(modifications / total_tokens) * 100 for modifications in n_modifications]\n",
    "    plt.plot(percentage_modifications, z_scores, \n",
    "             label=f'{attack_type.replace(\"_\", \" \").title()} Attack',\n",
    "             color=colors[i % len(colors)], \n",
    "             marker='o',\n",
    "             linewidth=2,\n",
    "             markersize=6)\n",
    "    \n",
    "plt.title('Tokenization Attack Z-Score vs. Percentage of Modifications')\n",
    "plt.xlabel('Percentage of Modifications (%)')\n",
    "plt.ylabel('Z-Score')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "\n",
    "for x in np.arange(min(percentage_modifications), max(percentage_modifications) + 1, 5):\n",
    "    plt.axvline(x=x, color='grey', linestyle='--', linewidth=0.5)\n",
    "for y in np.arange(0, max(max(z_scores) for z_scores in z_score_results.values()) + 0.25, 0.25):\n",
    "    plt.axhline(y=y, color='grey', linestyle='--', linewidth=0.5)\n",
    "rect = plt.Rectangle((0, 0), 1, 1, color='grey', alpha=0.2, transform=plt.gca().transAxes, zorder=-1)\n",
    "plt.gca().add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i, (attack_type, scores) in enumerate(robustness_scores.items()):\n",
    "    percentage_modifications = [(modifications / total_tokens) * 100 for modifications in n_modifications]\n",
    "    plt.plot(percentage_modifications, scores, \n",
    "             label=f'{attack_type.replace(\"_\", \" \").title()} Attack',\n",
    "             color=colors[i % len(colors)],\n",
    "             marker='o', \n",
    "             linewidth=2,  \n",
    "             markersize=6)  \n",
    "\n",
    "plt.title('Tokenization Attack Robustness Score vs. Percentage of Modifications')\n",
    "plt.xlabel('Percentage of Modifications (%)')\n",
    "plt.ylabel('Robustness Score')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "for x in np.arange(min(percentage_modifications), max(percentage_modifications) + 1, 5):\n",
    "    plt.axvline(x=x, color='grey', linestyle='--', linewidth=0.5)\n",
    "for y in np.arange(0, max(max(scores) for scores in robustness_scores.values()) + 0.25, 0.25):\n",
    "    plt.axhline(y=y, color='grey', linestyle='--', linewidth=0.5)\n",
    "rect = plt.Rectangle((0, 0), 1, 1, color='grey', alpha=0.2, transform=plt.gca().transAxes, zorder=-1)\n",
    "plt.gca().add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "paraphraser = ParaphrasingAttack(access_token=access_token)\n",
    "\n",
    "attacked_text = paraphraser.rephrase(content)\n",
    "non_inference_attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "attacked_text = paraphraser.rephrase(content, topic='sports', inference=True)\n",
    "relevant_inference_attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "\n",
    "attacked_text = paraphraser.rephrase(content, topic='health', inference=True)\n",
    "similar_inference_attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "\n",
    "attacked_text = paraphraser.rephrase(content, topic='technology', inference=True)\n",
    "non_relevant_inference_attacked_detection_result = detect(input_prompt, attacked_text, args, device=device, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "print(f'Unmodified Watermarked Text Metrics: {without_watermark_detection_result}')\n",
    "print(f'Generic Paraphrasing: {non_inference_attacked_detection_result}')\n",
    "print(f'Inference with Correct Topic Paraphrasing: {relevant_inference_attacked_detection_result}')\n",
    "print(f'Inference with Similar Topic Paraphrasing: {similar_inference_attacked_detection_result}')\n",
    "print(f'Inference with Incorrect Topic Paraphrasing: {non_relevant_inference_attacked_detection_result}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
