{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import load_model, generate, detect\n",
    "from topic_extractions import llm_topic_extraction\n",
    "from inputs import sports_input, technology_input, animals_input, music_input, medicine_input\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_mappings():\n",
    "    total_tokens = 100000\n",
    "    topics = [\"sports\", \"animals\", \"technology\", \"music\", \"medicine\"]\n",
    "\n",
    "    topic_token_mapping = {topic: [] for topic in topics}\n",
    "\n",
    "    for i in range(total_tokens):\n",
    "        topic_index = i % len(topics)\n",
    "        topic = topics[topic_index]\n",
    "        topic_token_mapping[topic].append(i)\n",
    "    return topic_token_mapping\n",
    "token_mappings = get_token_mappings()\n",
    "\n",
    "args = {\n",
    "    'demo_public': False, \n",
    "    'model_name_or_path': 'facebook/opt-1.3b', \n",
    "    'load_fp16' : False,\n",
    "    'prompt_max_length': None, \n",
    "    'max_new_tokens': 200, \n",
    "    'generation_seed': 123, \n",
    "    'use_sampling': True, \n",
    "    'n_beams': 1, \n",
    "    'sampling_temp': 0.7, \n",
    "    'use_gpu': True, \n",
    "    'seeding_scheme': 'simple_1', \n",
    "    'gamma': 0.4,\n",
    "    'delta': 2.0, \n",
    "    'normalizers': '', \n",
    "    'ignore_repeated_bigrams': False, \n",
    "    'detection_z_threshold': 4.0, \n",
    "    'select_green_tokens': True,\n",
    "    'skip_model_load': False,\n",
    "    'seed_separately': True,\n",
    "    'is_topic': True,\n",
    "    'topic_token_mapping': token_mappings,\n",
    "    'detected_topic': \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = sports_input()\n",
    "\n",
    "args['normalizers'] = (args['normalizers'].split(\",\") if args['normalizers'] else [])\n",
    "\n",
    "model, tokenizer = load_model(args)\n",
    "\n",
    "if args['is_topic']:\n",
    "    detected_topics = llm_topic_extraction(input_text)\n",
    "else:\n",
    "    detected_topics = []\n",
    "\n",
    "if DEBUG: print(f\"Topic extraction is finished for watermarking: {detected_topics}\")\n",
    "\n",
    "print(f\"Prompt:\\n {input_text}\")\n",
    "\n",
    "redecoded_input, truncation_warning, decoded_output_without_watermark, decoded_output_with_watermark = generate(\n",
    "    input_text, \n",
    "    detected_topics,\n",
    "    args, \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "if DEBUG: print(\"Decoding with and without watermarkings are finished\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_without_watermark\n",
    "\n",
    "without_watermark_detection_result = detect(input_prompt, decoded_output_without_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished without watermark detection\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_with_watermark\n",
    "\n",
    "with_watermark_detection_result = detect(input_prompt, decoded_output_with_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished with watermark detection\")\n",
    "\n",
    "\n",
    "print(\"#########################################\")\n",
    "print(\"Output without watermark:\")\n",
    "print(decoded_output_without_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(without_watermark_detection_result)\n",
    "print((\"#########################################\"))\n",
    "\n",
    "print((\"#########################################\"))\n",
    "print(\"Output with watermark:\")\n",
    "print(decoded_output_with_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(with_watermark_detection_result)\n",
    "print((\"#########################################\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = technology_input()\n",
    "\n",
    "args['normalizers'] = (args['normalizers'].split(\",\") if args['normalizers'] else [])\n",
    "\n",
    "model, tokenizer = load_model(args)\n",
    "\n",
    "if args['is_topic']:\n",
    "    detected_topics = llm_topic_extraction(input_text)\n",
    "else:\n",
    "    detected_topics = []\n",
    "\n",
    "if DEBUG: print(f\"Topic extraction is finished for watermarking: {detected_topics}\")\n",
    "\n",
    "print(f\"Prompt:\\n {input_text}\")\n",
    "\n",
    "redecoded_input, truncation_warning, decoded_output_without_watermark, decoded_output_with_watermark = generate(\n",
    "    input_text, \n",
    "    detected_topics,\n",
    "    args, \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "if DEBUG: print(\"Decoding with and without watermarkings are finished\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_without_watermark\n",
    "\n",
    "without_watermark_detection_result = detect(input_prompt, decoded_output_without_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished without watermark detection\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_with_watermark\n",
    "\n",
    "with_watermark_detection_result = detect(input_prompt, decoded_output_with_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished with watermark detection\")\n",
    "\n",
    "\n",
    "print(\"#########################################\")\n",
    "print(\"Output without watermark:\")\n",
    "print(decoded_output_without_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(without_watermark_detection_result)\n",
    "print((\"#########################################\"))\n",
    "\n",
    "print((\"#########################################\"))\n",
    "print(\"Output with watermark:\")\n",
    "print(decoded_output_with_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(with_watermark_detection_result)\n",
    "print((\"#########################################\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = animals_input()\n",
    "\n",
    "args['normalizers'] = (args['normalizers'].split(\",\") if args['normalizers'] else [])\n",
    "\n",
    "model, tokenizer = load_model(args)\n",
    "\n",
    "if args['is_topic']:\n",
    "    detected_topics = llm_topic_extraction(input_text)\n",
    "else:\n",
    "    detected_topics = []\n",
    "\n",
    "if DEBUG: print(f\"Topic extraction is finished for watermarking: {detected_topics}\")\n",
    "\n",
    "print(f\"Prompt:\\n {input_text}\")\n",
    "\n",
    "redecoded_input, truncation_warning, decoded_output_without_watermark, decoded_output_with_watermark = generate(\n",
    "    input_text, \n",
    "    detected_topics,\n",
    "    args, \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "if DEBUG: print(\"Decoding with and without watermarkings are finished\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_without_watermark\n",
    "\n",
    "without_watermark_detection_result = detect(input_prompt, decoded_output_without_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished without watermark detection\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_with_watermark\n",
    "\n",
    "with_watermark_detection_result = detect(input_prompt, decoded_output_with_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished with watermark detection\")\n",
    "\n",
    "\n",
    "print(\"#########################################\")\n",
    "print(\"Output without watermark:\")\n",
    "print(decoded_output_without_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(without_watermark_detection_result)\n",
    "print((\"#########################################\"))\n",
    "\n",
    "print((\"#########################################\"))\n",
    "print(\"Output with watermark:\")\n",
    "print(decoded_output_with_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(with_watermark_detection_result)\n",
    "print((\"#########################################\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = music_input()\n",
    "\n",
    "args['normalizers'] = (args['normalizers'].split(\",\") if args['normalizers'] else [])\n",
    "\n",
    "model, tokenizer = load_model(args)\n",
    "\n",
    "if args['is_topic']:\n",
    "    detected_topics = llm_topic_extraction(input_text)\n",
    "else:\n",
    "    detected_topics = []\n",
    "\n",
    "if DEBUG: print(f\"Topic extraction is finished for watermarking: {detected_topics}\")\n",
    "\n",
    "print(f\"Prompt:\\n {input_text}\")\n",
    "\n",
    "redecoded_input, truncation_warning, decoded_output_without_watermark, decoded_output_with_watermark = generate(\n",
    "    input_text, \n",
    "    detected_topics,\n",
    "    args, \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "if DEBUG: print(\"Decoding with and without watermarkings are finished\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_without_watermark\n",
    "\n",
    "without_watermark_detection_result = detect(input_prompt, decoded_output_without_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished without watermark detection\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_with_watermark\n",
    "\n",
    "with_watermark_detection_result = detect(input_prompt, decoded_output_with_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished with watermark detection\")\n",
    "\n",
    "\n",
    "print(\"#########################################\")\n",
    "print(\"Output without watermark:\")\n",
    "print(decoded_output_without_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(without_watermark_detection_result)\n",
    "print((\"#########################################\"))\n",
    "\n",
    "print((\"#########################################\"))\n",
    "print(\"Output with watermark:\")\n",
    "print(decoded_output_with_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(with_watermark_detection_result)\n",
    "print((\"#########################################\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = medicine_input()\n",
    "\n",
    "args['normalizers'] = (args['normalizers'].split(\",\") if args['normalizers'] else [])\n",
    "\n",
    "model, tokenizer = load_model(args)\n",
    "\n",
    "if args['is_topic']:\n",
    "    detected_topics = llm_topic_extraction(input_text)\n",
    "else:\n",
    "    detected_topics = []\n",
    "\n",
    "if DEBUG: print(f\"Topic extraction is finished for watermarking: {detected_topics}\")\n",
    "\n",
    "print(f\"Prompt:\\n {input_text}\")\n",
    "\n",
    "redecoded_input, truncation_warning, decoded_output_without_watermark, decoded_output_with_watermark = generate(\n",
    "    input_text, \n",
    "    detected_topics,\n",
    "    args, \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "if DEBUG: print(\"Decoding with and without watermarkings are finished\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_without_watermark\n",
    "\n",
    "without_watermark_detection_result = detect(input_prompt, decoded_output_without_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished without watermark detection\")\n",
    "\n",
    "input_prompt = input_text + decoded_output_with_watermark\n",
    "\n",
    "with_watermark_detection_result = detect(input_prompt, decoded_output_with_watermark, \n",
    "                                            args, \n",
    "                                            device=device, \n",
    "                                            tokenizer=tokenizer)\n",
    "if DEBUG: print(\"Finished with watermark detection\")\n",
    "\n",
    "\n",
    "print(\"#########################################\")\n",
    "print(\"Output without watermark:\")\n",
    "print(decoded_output_without_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(without_watermark_detection_result)\n",
    "print((\"#########################################\"))\n",
    "\n",
    "print((\"#########################################\"))\n",
    "print(\"Output with watermark:\")\n",
    "print(decoded_output_with_watermark)\n",
    "print((\"#########################################\"))\n",
    "print(f\"Detection result @ {args['detection_z_threshold']}:\")\n",
    "pprint(with_watermark_detection_result)\n",
    "print((\"#########################################\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
